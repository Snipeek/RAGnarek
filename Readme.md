Название RAGнарек на Llama'х и не только говорит что я хочу рассказать про последние подходы которые есть в нейронках. Начиная с какой-то своей истории, что я мельком услышал о нейронках, потом знакомый мне рассказал про bert, но я так и не мог понять что мне делать и казалось что мне понадобится много денег чтобы начать с этим работать и поэтому долго не погружался в этот процесс

За несколько лет индустрия развилась и появились трансформеры, которые дали возможность создать chatgpt, что максимально всех приблизило к слову "Умный" в продуктах которые все создают, поэтмоу все массово начали внедрять к себе. 

Из чего состоит это внедрение: все знают про то что есть API, посмотрим как им пользоваться, в каких кейсах может пригодиться

Главным минусом подхода является то что мы отправляем данные куда то на сервер. Поэтому в основном такой подход прикольно использовать в рамках небольшого контекста или так сказать диалога. Наприример когда мы хотим суммаризировать информацию (пример коллеги, который сделал бота котоырый помогает побыстрому описывать чем он занимался за текущий день на основе фотографий питания и рецептов которые он туда скидывает)

Но вот у нам приходит предложение создать что-то на основе персональных данных и мы понимаем что если мы их будем отправлять не внешний сервер - это сравни разглашению, потому что они могут у себя все логировать и дообучаться на этих данных

Большие корпорации могут позволить себе раз в какой-то момент дообучать сеть на основе своих данных. Так называемый файнтюнинг - посмотрим как он обычно устроен, как его можно устроить на питоне и если получится на nodejs. Скажем что любое дообучение стоит дорого, приведем примеры количество ресурсов которые нужны чтобы обучить chat-gpt и сколько нужно чтобы их дообучить.

Но какие-то маленькие компании или энтузиасты не имеют таких ресурсов, а еще нам не гарантирован результат, поэтому это несколько итераций.

Тут мы подходим к работе, которая пытается описать подход RAG, и сравнивая результаты приходит к тому что сейчас контекстное окно позволяет закидывать контекст вопроса с самим вопросом, потому что все нейронки уже умеет выстраивать на основе заданного текста конечный результат. им только важно обладать контекстом, но скармливать огромное количество инфы тоже сложно. Поэтому нам нужен какой-то поиск по всей инфе что у нас есть. В целом это может быть и полнотекстовый поиск, который выдергивает параграфы, но у нас также есть векторный поиск по ембендингам

Тут мы рассказываем что такое ембединг и почему он является базовой частью всего датасаинса, что такое эти веса, как он определяет смысл который зашит в тексте и что все это на самом деле родилось с клавиатуры, когда вбивая пару символов мы пытались предугадать какое слово мы хотим вбить

Показываем апи и говорим что там есть специальй эндпоинт под это дело. И технически мы можем сложить это к себе в базу данных. Рассказываю про ветокрный поиск и редис, который умеет в это. Модель для ембендинга может быть легче чем модель которая дает ответб так как основная цель это поиск нужных фрагментов, а не генерация ответа.

Основная проблема в вычислениях, их должно быть много. Поэтому основной бум всех нейронок связан с развитием видео карт, показываю почему они эффективные. Но это нужно только на этапе обучения. На этапе поднятие нам уже не нужны эти петобайты ресурсов. Показываю что нам нужна видео память соответственная размеру сети. Поэтому комп который будет это запускать стоит столько то.

Умные ребята придумали сделать адаптер который позволяет все тоже самое запускать и на процессоре. Показываю llama-cpp, рассказываю что такое llama, что она стала одной из опенсорных моделек которые могут все использовать, считаем что с этого стартовал бум на открытость систем и борьбу

Показываю что есть node-llama-cpp, который обладаем тем же api, что и chat-gpt и мы можем это запускать через nodejs и оборачивать в api.

Мы можем сэкономить на модельках через квантизацию, которая уменьшает вес большой модельки. РАссказываю про нее и чем сулит

Показываю итог где мы запустили на основе книжек собственного помощника, рассказываю как работают нейро и что это тот же самый подход с RAG скорее всего, потому что каждый раз дообучаться сложно

Теперь мы разобрались как потихоньку входить в тему llm через апи и плавно уходить на собственное железо, которое дает нам возможность уже генерировать все обилие фичей которыми когда-то обладали только большие компании